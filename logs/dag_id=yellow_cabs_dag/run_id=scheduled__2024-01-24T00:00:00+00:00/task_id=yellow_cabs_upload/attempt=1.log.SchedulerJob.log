[2024-01-26T20:02:50.496+0000] {scheduler_job_runner.py:1586} WARNING - Marking task instance <TaskInstance: yellow_cabs_dag.yellow_cabs_upload scheduled__2024-01-24T00:00:00+00:00 [queued]> stuck in queued as failed. If the task instance has available retries, it will be retried.
[2024-01-26T20:02:50.633+0000] {scheduler_job_runner.py:781} ERROR - Executor reports task instance <TaskInstance: yellow_cabs_dag.yellow_cabs_upload scheduled__2024-01-24T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-01-29T18:42:26.832+0000] {scheduler_job_runner.py:1766} ERROR - Detected zombie job: {'full_filepath': '/opt/airflow/dags/yellow_cabs_project.py', 'processor_subdir': '/opt/airflow/dags', 'msg': "{'DAG Id': 'yellow_cabs_dag', 'Task Id': 'yellow_cabs_upload', 'Run Id': 'scheduled__2024-01-24T00:00:00+00:00', 'Hostname': 'd8c0631e3b56', 'External Executor Id': '196e5240-d9c1-49e1-ade2-3811b90a2d36'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0xffff74415c40>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)
